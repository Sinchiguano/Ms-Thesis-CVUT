
\chapter{Experimental Results}
\label{chap:exp}

This chapter presents the experiments and the results of the evaluations to the propose robot-camera calibration method as well as to the pose estimation system. In addition to those, an evaluation of the accuracy of the cameras used in this thesis is presented. In order to make such evaluation, the reprojection error, a well understood and known metric in the field of computer vision is used. The reprojection error is an import and crucial step which can determine how accurate the robot-camera calibration can be, for that reason it is performed first, followed by the evaluation of the robot-camera calibration which is essential and determinant step to have a good performance in the pose estimation system, which is the last one to undergo an evaluation process as well.

\section{Robot-Camera Calibration on the Yumi robot}
In order to get an accurate estimation of the robot-camera pose depends on how well the estimation of the intrinsic parameters related to the camera can be. Such estimation of those parameters was accomplished as described in section \ref{pppp}, where those existing algorithms were used for checkerboard detection in both colour and depth data. With those parameter knowing in advance, a reprojection error is calculated for both cameras. 

\subsubsection{Reprojection Error}

The reprojection error is the distance between a pattern keypoint detected in a calibration image, and a corresponding world point projected into the same image. Figure \ref{fig:realopen} and Figure \ref{fig:realros} show the calibration results by analyzing the reprojection error per image for the RealSense camera when OpenCV and camera \textunderscore industrial calibration were used. Figure \ref{fig:astraopen} and Figure \ref{fig:astraros} show the calibration results by analyzing the reprojection error per image for the Astra camera when the OpenCV and camera \textunderscore industrial calibration were used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
\begin{center}
\includegraphics[width=5in, height=3.5in]{figures05/ros_int_cal_astra.png}
\caption{Mean Reprojection Error per image with a ROS method (Astra Orbbec Camera)}%\cite{temp2}}
\label{fig:astraros}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=5in, height=3.5in]{figures05/ros_int_cal_real.png}
\caption{Mean Reprojection Error per image with a ROS method (RealSense D-435)}%\cite{temp2}}
\label{fig:realros}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=5in, height=3.5in]{figures05/opencv_int_cal_astra.png}
\caption{Mean Reprojection Error per image with a OpenCV method (Astra Orbbec Camera)}%\cite{temp2}}
\label{fig:astraopen}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=5in, height=3.5in]{figures05/opencv_int_cal_real.png}
\caption{Mean Reprojection Error per image with a OpenCV method (RealSense D-435)}%\cite{temp2}}
\label{fig:realopen}
\end{center}
\end{figure}

To find the average error we calculate the arithmetical mean of the errors calculates for all the calibration images. This should be as close to zero as possible. 

\subsubsection{Result Analysis}
After computing the average error for both cameras, the results are shown in Table \ref{astra1} related to the Astra camera and Table \ref{real1} which corresponds to the RealSense D-435. It can be seen that the overall mean error representing the Astra camera is quite correlated to one another method (OpenCV or camera \textunderscore industrial calibration). The difference is acceptable since it is under the standard specified in most of the literature of computer vision as it is that the error should as close as possible to 0, or under the 0.5 pixels.\\
On the other hand, the results for the RealSense camera are not quite correlated at all. There exists a difference that can affect the eye-to-hand calibration process, and therefore, the pose estimation system can be affected also. Even when the overall mean error meets the requirements of being under the 0,5 pixels, it is difficult to conclude that the RealSense camera is or is not a good choice for solving the main problem of this thesis as it is the pose estimation system. But, it is a good insight to know in advance for future conclusions where there might exist potential issues in the development of this master thesis.
As a remark, both cameras were calibrated with same light conditions, with the same calibration target and with an equal number of images. Considering, all in all, it can be concluded that the Astra camera seems to perform well and it can be our final choice for the pose estimation system.

\begin{table}[b]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Experiment data for internal Astra sensor calibration.}
\label{astra1}
\centering
\begin{tabular}{|c||c|}
\hline
Method & Overall Mean Error\\
\hline
OpenCV &  0.1041954808\\
\hline
ROS &  0.1081118023\\
\hline
\hline
\end{tabular}
\end{table}

\begin{table}[b]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Experiment data for internal RealSense sensor calibration.}
\label{real1}
\centering
\begin{tabular}{|c||c|}
\hline
Method & Overall Mean Error\\
\hline
OpenCV &  0.1684388411\\
\hline
ROS &  0.122868849\\
\hline
\hline
\end{tabular}
\end{table}

\subsection{Eye-To-Hand Calibration}
The second experiment in this chapter is related to the eye-to-hand calibration. In this experiment, the Astra and RealSense cameras were used. Since, the quality of the extrinsic parameters depends on how good the estimation of the intrinsic parameters are, the internal parameters were selected carefully by analizing the reprojection error of each individual camera as mentioned previously. The less reprojection error the camera produces, the more accurate the camera is. \\
By defining the most accurate internal parameter for each one of the camera respectively. The experiment to be performed are divided in two types.
In the first type of experiment, where the calibration plate is kept at a constant angle, parallel to the XY plane of the robot coordinate system as it is shown in Figure \ref{iiiii}, the robot proceed with execution of several predefined set of movements (translation) around the XY plane.\\ 
As to the second experiment, where the calibration plate is   tilting with predefined orientations and small translation around the XY plane of the robot frame, provided that the checkboard is always detected by the camera to be calibrated the robot execute the movement. In between of the movements, a pause is set in order to let the camera take a good quality of 2D image that later is used for the pose estimation of the camera.\\

In order to excute the movement of the robot arm and compute the estimation of the camera frame relative to the robot frame, three nodes were developed as described in \ref{jjjjjj}, where one of the node is for controlling the robot movement and publishing the transformation from the robot base frame to TCP (Tool Center Point) frame in the ROS network (tf topic to be specific). While the other node is for estimating the pose of the camera relative the to the calibration target frame and braodcasting this pose in the ROS networke. When it comes to the last node, which is responsible for keeping track of the all coordinate frames over time, and querying for the transformation of the camera frame relative to the robot frame. In addition to that, an average transformation is computed after each executed movement of the robot arm. Since the quality of robot-camera pose depends on the quality of the image taken, a pause of few seconds between movements in the robot control node is set in order to cancel the effects of possible vibrations that the robot can produce, so that wrong measurements are avoided.



\subsubsection{Calibration results}


%%%%%%%%%%%%% astra 




\begin{itemize}
\item The following eye-to-hand transform was obtained for the Astra Camera with the calibration plate parallel to the XY plane in robot frame:
\begin{equation}
^{R}T_{C}=\begin{bmatrix} -2.26051005e-02 & 7.30112611e-01 & -6.82952842e-01 & 1.19260379\\9.99481127e-01 & 8.25583772e-04 & -3.21992981e-02 & 0.12098781\\ -2.29452788e-02 & -6.83326345e-01 & -7.29752438e-01 & 0.53569317 \end{bmatrix}
\end{equation}


				
\item The following eye-to-hand transform was obtained for the Astra Camera by tilting the calibration plate:
\begin{equation}
^{R}T_{C}=\begin{bmatrix} -0.01440125 & 0.72431469 & -0.68931911 & 1.19518608\\0.99970886 & -0.00291747 & -0.02395149 & 0.11374275\\ -0.01935949 & -0.68946336 & -0.7240618 & 0.53606583 \end{bmatrix}
\end{equation}


%%%%%%%%%%%%%%%%% realsense

\item The following eye-to-hand transform was obtained for the RealSense D-435 camera with the calibration plate parallel to the XY plane in robot frame:
\begin{equation}
^{R}T_{C}=\begin{bmatrix} -2.23312402e-02 & 2.94145863e-01 & -9.55499622e-01 &1.22253213\\9.99723971e-01 & -4.09078692e-04 & -2.34907523e-02 & 0.11776472 \\ -7.30058216e-03 & -9.55760453e-01 & -2.94055535e-01 & 0.32424239  \end{bmatrix}
\end{equation}



\item The following eye-to-hand transform was obtained for the RealSense D-435 camera by tilting the calibration plate:
\begin{equation}
^{R}T_{C}=\begin{bmatrix} -0.01833338  0.30085851 -0.95349255 & 1.21972025\\0.99978158 -0.00405373 -0.02050249& 0.09386797\\ -0.01003355 -0.95366017 -0.30071848& 0.33192816\end{bmatrix}
\end{equation}
\end{itemize}


\subsubsection{Result Analysis}

The propose robot-camera calibration method was successfully performed provided that the calibration target was detected by the 3D camera to be calibrated. In order to validate whether the propose method is accurate enough for the pose estimation system discribed in \ref{oooooo} or not, a ground truth is neccesary to know in advance. But it was a challenging task to meassure an exact orientation and translation of the camera with respect to the robot frame. One of the several diffulties encounter was that the housing of the camera (namely the RealSense camera) is rounded which makes it hard to get good measurement of the mounting point to the camera. Another issue, the robot and the set of cameras were mounting on different tables such that a simple meassurement of distance was not possible to do it with a reasonable accuracy. In addition to that, the rotation of the sensor in the housing is also not known. A simple solution would be to use an industrial camera where knowing transformations are available. But this was not available. \\
The propuse solution needed to be evaluated somehow. Since a traditional error estimation for evalution is not possible, in this thesis a repeatability test is taken into account for the evalutation. The repeatability test consiste of run again the robot but with the major difference that the number of movements is increased. In addition that, the orientation and displacement of the calibration target are modified for each individual movement also. By doing the repeatability test an average translation and orientation is presented in \ref{uuu}.




%For astra camera
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[b]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Mean Values of The Repeatability Test With a Constant Orientation of the Calibration Plate(Astra Camera).}
\label{real1}
\centering
\begin{tabular}{|c||c||c||c||c||c||c|}
\hline
$x[m]$ & $y[m]$ & $z[m]$ & $q_{x}$ & $q_{y}$ & $q_{z}$ &$q_{\omega}$ \\
\hline
1.1926 & 0.1245 & 0.5368 & 3.0992939&-0.7519546&-1.60088 & \\
\hline
\hline
\end{tabular}
\end{table}


\begin{table}[b]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Mean Values of The Repeatability Test With Tilting Motion of the Calibration Plate(Astra Camera).}
\label{real1}
\centering
\begin{tabular}{|c||c||c||c||c||c||c|}
\hline
$x[m]$ & $y[m]$ & $z[m]$ & $q_{x}$ & $q_{y}$ & $q_{z}$ &$q_{\omega}$ \\
\hline
1.1970885638 & 0.1165706889 & 0.5395085367 & 3.1042422&-0.7573089&-1.5959978 & \\
\hline
\hline
\end{tabular}
\end{table}


%For real sense
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		
\begin{table}[b]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Mean Values of The Repeatability Test With a Constant Orientation of the Calibration Plate(Astra Camera).}
\label{real1}
\centering
\begin{tabular}{|c||c||c||c||c||c||c|}
\hline
$x[m]$ & $y[m]$ & $z[m]$ & $q_{x}$ & $q_{y}$ & $q_{z}$ &$q_{\omega}$ \\
\hline
1.1926043293 & 0.1244785302 & 0.5367988515 & 3.0992939&-0.7519546&-1.60088 & \\
\hline
\hline
\end{tabular}
\end{table}


\begin{table}[b]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Mean Values of The Repeatability Test With Tilting Motion of the Calibration Plate(Astra Camera).}
\label{real1}
\centering
\begin{tabular}{|c||c||c||c||c||c||c|}
\hline
$x[m]$ & $y[m]$ & $z[m]$ & $q_{x}$ & $q_{y}$ & $q_{z}$ &$q_{\omega}$ \\
\hline
1.1970885638 & 0.1165706889 & 0.5395085367 & 3.1042422&-0.7573089&-1.5959978  \\
\hline
\hline
\end{tabular}
\end{table}


		











\section{Pose Estimation Pipeline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
%%% not yet
 The principle is that the checkerboard is detected, the pose of camera relative to the chessboard frame and the pose of the checkerboard relative to the robot frame is being streamed through the tf topic of  the ROS network, then a pose of the camera relative to the robot frame is retrieved with by using tf listening transform. This transformation is saved. The cycle repeats for each one of the movement the robot has to execute. The number of successful detection of the checkerboard is saved also. When the robot has completed all the movements, an average of all poses is computed. 

As to, the robot control, several packages from the open source community were tested with the idea in mind to interface the robot with a ros node.  Since the whole system was developed in ROS, which offers huge support to most of the sensors and robots, a rapid development can be done. 
With the two types of experiment to handle and how the communication with the robot is hold. 


\begin{itemize}
\item Internal camera calibration where a data collection is requiring.

\end{equation}
\item Eye-to-hand calibration where a robot move is requiring. 

\end{itemize}

The propose robot-camera calibration method was successfully performed provided that the checkerboard was detected by the 3D camera to be calibrated.


 Results are divided in two sections, each one requiring an independent set. For an intrinsic calibration, since the extrinsic calibration depends on the known intrinsic parameteres of the camera.  it is not required that the calibration plate be attached to the robot gripper. As to, the extrinsic calibration, the calibration target which is fixed on the customed-made plate is attached to the gripper of the robot.

\subsection{Dataset}

For the performance evaluation, a dataset of scenes was collected. It contains 60 point clouds wich can be classified as:
\begin{itemize}
\item point clouds, captured in a synthetic environment.
\item point clouds, captured in real environment. 
\end{itemize}

\subsection{Metrics}
In this thesis two standard metrics are used in order to evaluate the performance of the pose estimation algorithms.
\begin{itemize}
\item Precision:
\item point clouds, captured in real environment. 
\end{itemize}

\subsection{Testing environment}
The robot-camera calibration method as well as the pose estimation system were implemented in Python within ROS enviornment, making use of highly optimized libraries for 3D point cloud processing and tools. 

\subsection{Hardware}
All tests for which we report the expirements and result have been made using a personal computer Lenovo ThinkPad T430, Intel Core i5-3320 CPU 2.60GHz x 4, 7.8GB RAM, Ubuntu 16.04 64-bit OS
and ROS Kinetic release.

\fi